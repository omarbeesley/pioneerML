{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366b189b",
   "metadata": {},
   "source": [
    "\n",
    "# Positron Angle Predictor\n",
    "\n",
    "Training utilities for predicting the positron emission direction from time-group graphs, including per-epoch angular-error histograms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27378738",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = '/mnt/c/Users/obbee/research/notebooks/ML'\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "    os.environ['PYTHONPATH'] = PROJECT_ROOT + os.pathsep + os.environ.get('PYTHONPATH', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from graph_data.utils import PositronAngleGraphDataset, PositronAngleRecord\n",
    "from graph_data.models import PositronAngleModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b41604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_angle_records(\n",
    "    groups: Sequence[Iterable],\n",
    "    angle_targets: Sequence[Sequence[float]],\n",
    "    *,\n",
    "    event_ids: Optional[Sequence[int]] = None,\n",
    "    pion_stops: Optional[Sequence[Sequence[float]]] = None,\n",
    ") -> List[PositronAngleRecord]:\n",
    "    \"\"\"Normalize raw group arrays plus per-group angle labels into PositronAngleRecord objects.\"\"\"\n",
    "    if len(groups) != len(angle_targets):\n",
    "        raise ValueError(\"angle_targets length must match groups length\")\n",
    "\n",
    "    records: List[PositronAngleRecord] = []\n",
    "    for idx, (group, angle) in enumerate(zip(groups, angle_targets)):\n",
    "        arr = np.asarray(group)\n",
    "        if arr.ndim != 2 or arr.shape[0] < 2 or arr.shape[1] < 4:\n",
    "            raise ValueError(f\"Group at index {idx} must be [N, >=4], got shape {arr.shape}\")\n",
    "\n",
    "        record_event = int(event_ids[idx]) if event_ids is not None else None\n",
    "        pion_stop = None if pion_stops is None else np.asarray(pion_stops[idx], dtype=np.float32)\n",
    "\n",
    "        records.append(PositronAngleRecord(\n",
    "            coord=arr[:, 0].astype(np.float32),\n",
    "            z=arr[:, 1].astype(np.float32),\n",
    "            energy=arr[:, 2].astype(np.float32),\n",
    "            view=arr[:, 3].astype(np.float32),\n",
    "            angle=angle,\n",
    "            event_id=record_event,\n",
    "            group_id=idx,\n",
    "            pion_stop=None if pion_stop is None else pion_stop.astype(np.float32),\n",
    "        ))\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970da600",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _split_records(\n",
    "    records: Sequence[PositronAngleRecord | Dict[str, Any]],\n",
    "    *,\n",
    "    train_fraction: float = 0.85,\n",
    "    seed: int = 13,\n",
    ") -> Tuple[List[PositronAngleRecord | Dict[str, Any]], List[PositronAngleRecord | Dict[str, Any]]]:\n",
    "    if not records:\n",
    "        return [], []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    indices = np.arange(len(records))\n",
    "    rng.shuffle(indices)\n",
    "    if len(indices) == 1:\n",
    "        return [records[int(indices[0])]], []\n",
    "    split = int(len(indices) * train_fraction)\n",
    "    split = min(max(split, 1), len(indices) - 1)\n",
    "    train_idx = indices[:split]\n",
    "    val_idx = indices[split:]\n",
    "    return [records[i] for i in train_idx], [records[i] for i in val_idx]\n",
    "\n",
    "\n",
    "def _make_loaders(\n",
    "    records: Sequence[PositronAngleRecord | Dict[str, Any]],\n",
    "    *,\n",
    "    batch_size: int,\n",
    "    train_fraction: float,\n",
    "    seed: int,\n",
    "):\n",
    "    train_records, val_records = _split_records(records, train_fraction=train_fraction, seed=seed)\n",
    "    if not train_records:\n",
    "        raise ValueError(\"Training set is empty. Check filtering parameters.\")\n",
    "\n",
    "    train_dataset = PositronAngleGraphDataset(train_records)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_loader = None\n",
    "    val_dataset = None\n",
    "    if val_records:\n",
    "        val_dataset = PositronAngleGraphDataset(val_records)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def _angle_errors_deg(preds: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    preds_n = F.normalize(preds, dim=1)\n",
    "    targets_n = F.normalize(targets, dim=1)\n",
    "    cos_sim = torch.clamp((preds_n * targets_n).sum(dim=1), -0.999999, 0.999999)\n",
    "    return torch.arccos(cos_sim) * (180.0 / math.pi)\n",
    "\n",
    "\n",
    "def _angle_histogram(errors: np.ndarray | Sequence[float], bins: int = 50, span: Tuple[float, float] = (0.0, 180.0)):\n",
    "    values = np.asarray(errors, dtype=np.float32)\n",
    "    if values.size == 0:\n",
    "        return None\n",
    "    counts, edges = np.histogram(values, bins=bins, range=span)\n",
    "    return {\n",
    "        'counts': counts.tolist(),\n",
    "        'bin_edges': edges.tolist(),\n",
    "    }\n",
    "\n",
    "\n",
    "def _plot_histogram(title: str, hist_data: Optional[Dict[str, Any]]) -> None:\n",
    "    if hist_data is None:\n",
    "        return\n",
    "    counts = np.asarray(hist_data['counts'], dtype=np.float32)\n",
    "    edges = np.asarray(hist_data['bin_edges'], dtype=np.float32)\n",
    "    if counts.size == 0 or edges.size == 0:\n",
    "        return\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "    widths = np.diff(edges)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    for ax, log_scale in zip(axes, [False, True]):\n",
    "        ax.bar(centers, counts, width=widths, align='center', alpha=0.75, color='crimson')\n",
    "        ax.set_title(f\"{title} angle error histogram\" + (' (log)' if log_scale else ''))\n",
    "        ax.set_xlabel('Angular error [deg]')\n",
    "        ax.set_ylabel('Counts')\n",
    "        ax.set_xlim(0.0, 180.0)\n",
    "        if log_scale:\n",
    "            ax.set_yscale('log')\n",
    "        ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _run_train_epoch(model, loader, optimizer, loss_fn, device, grad_clip=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_samples = 0\n",
    "    angle_values: List[torch.Tensor] = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        target = data.y.view(data.num_graphs, -1).float()\n",
    "        preds = model(data)\n",
    "        loss = loss_fn(preds, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if grad_clip is not None and grad_clip > 0:\n",
    "            clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_errors = _angle_errors_deg(preds.detach(), target)\n",
    "        angle_values.append(batch_errors.cpu())\n",
    "\n",
    "        batch_size = target.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_error += batch_errors.sum().item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "    mean_loss = total_loss / max(total_samples, 1)\n",
    "    mean_error = total_error / max(total_samples, 1)\n",
    "    all_errors = torch.cat(angle_values).numpy() if angle_values else np.zeros(0, dtype=np.float32)\n",
    "    return mean_loss, mean_error, all_errors\n",
    "\n",
    "\n",
    "def _run_eval_epoch(model, loader, loss_fn, device):\n",
    "    if loader is None:\n",
    "        return math.nan, math.nan, np.zeros(0, dtype=np.float32)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_samples = 0\n",
    "    angle_values: List[torch.Tensor] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            target = data.y.view(data.num_graphs, -1).float()\n",
    "            preds = model(data)\n",
    "            loss = loss_fn(preds, target)\n",
    "\n",
    "            batch_errors = _angle_errors_deg(preds, target)\n",
    "            angle_values.append(batch_errors.cpu())\n",
    "\n",
    "            batch_size = target.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_error += batch_errors.sum().item()\n",
    "            total_samples += batch_size\n",
    "\n",
    "    mean_loss = total_loss / max(total_samples, 1)\n",
    "    mean_error = total_error / max(total_samples, 1)\n",
    "    all_errors = torch.cat(angle_values).numpy() if angle_values else np.zeros(0, dtype=np.float32)\n",
    "    return mean_loss, mean_error, all_errors\n",
    "\n",
    "\n",
    "def train_positron_angle_predictor(\n",
    "    records: Sequence[PositronAngleRecord | Dict[str, Any]],\n",
    "    *,\n",
    "    model: Optional[torch.nn.Module] = None,\n",
    "    batch_size: int = 128,\n",
    "    epochs: int = 20,\n",
    "    lr: float = 5e-4,\n",
    "    weight_decay: float = 1e-5,\n",
    "    train_fraction: float = 0.85,\n",
    "    seed: int = 13,\n",
    "    grad_clip: Optional[float] = 2.0,\n",
    "    scheduler_step_size: Optional[int] = None,\n",
    "    scheduler_gamma: float = 0.7,\n",
    "    device: Optional[torch.device | str] = None,\n",
    "):\n",
    "    if model is None:\n",
    "        model = PositronAngleModel()\n",
    "\n",
    "    device_obj = torch.device(device) if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device_obj)\n",
    "\n",
    "    train_loader, val_loader, train_dataset, val_dataset = _make_loaders(\n",
    "        records,\n",
    "        batch_size=batch_size,\n",
    "        train_fraction=train_fraction,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler = None\n",
    "    if scheduler_step_size is not None and scheduler_step_size > 0:\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "\n",
    "    history = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        train_loss, train_err, train_errors = _run_train_epoch(model, train_loader, optimizer, loss_fn, device_obj, grad_clip)\n",
    "        val_loss, val_err, val_errors = _run_eval_epoch(model, val_loader, loss_fn, device_obj)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_hist = _angle_histogram(train_errors)\n",
    "        val_hist = _angle_histogram(val_errors) if not math.isnan(val_loss) else None\n",
    "\n",
    "        _plot_histogram('Train', train_hist)\n",
    "        if val_hist is not None:\n",
    "            _plot_histogram('Validation', val_hist)\n",
    "\n",
    "        if not math.isnan(val_loss):\n",
    "            print(\n",
    "                f\"Epoch {epoch:02d} | lr={current_lr:.5f} | train_loss={train_loss:.5f} err={train_err:.3f}° | \"\n",
    "                f\"val_loss={val_loss:.5f} err={val_err:.3f}°\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"Epoch {epoch:02d} | lr={current_lr:.5f} | train_loss={train_loss:.5f} err={train_err:.3f}°\"\n",
    "            )\n",
    "\n",
    "        history.append({\n",
    "            'epoch': epoch,\n",
    "            'lr': float(current_lr),\n",
    "            'train_loss': float(train_loss),\n",
    "            'train_error_deg': float(train_err),\n",
    "            'train_histogram': train_hist,\n",
    "            'val_loss': None if math.isnan(val_loss) else float(val_loss),\n",
    "            'val_error_deg': None if math.isnan(val_err) else float(val_err),\n",
    "            'val_histogram': val_hist,\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'train_size': len(train_dataset),\n",
    "        'val_size': 0 if val_dataset is None else len(val_dataset),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67850282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "# raw_groups = [...]  # sequence of per-group arrays\n",
    "# angle_targets = [...]  # list of [theta, phi] (radians) or unit vectors [x, y, z]\n",
    "# records = build_angle_records(raw_groups, angle_targets)\n",
    "#\n",
    "# model = PositronAngleModel(in_channels=5, hidden=150, heads=4, layers=3, dropout=0.1)\n",
    "#\n",
    "# results = train_positron_angle_predictor(\n",
    "#     records,\n",
    "#     model=model,\n",
    "#     batch_size=64,\n",
    "#     epochs=20,\n",
    "#     lr=5e-4,\n",
    "#     weight_decay=1e-5,\n",
    "#     train_fraction=0.85,\n",
    "#     seed=42,\n",
    "#     grad_clip=2.0,\n",
    "#     scheduler_step_size=10,\n",
    "#     scheduler_gamma=0.6,\n",
    "# )\n",
    "#\n",
    "# print(f\"Train groups: {results['train_size']} | Val groups: {results['val_size']}\")\n",
    "# for entry in results['history']:\n",
    "#     val_part = ''\n",
    "#     if entry['val_loss'] is not None:\n",
    "#         val_part = f\" | val_loss={entry['val_loss']:.5f} err={entry['val_error_deg']:.3f}°\"\n",
    "#     print(\n",
    "#         f\"Epoch {entry['epoch']:02d}: train_loss={entry['train_loss']:.5f} err={entry['train_error_deg']:.3f}°{val_part}\"\n",
    "#     )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
